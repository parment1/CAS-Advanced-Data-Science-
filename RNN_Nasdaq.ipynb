{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parment1/CAS-Advanced-Data-Science-/blob/main/RNN_Nasdaq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "004492b4",
      "metadata": {
        "id": "004492b4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from yahoo_fin import stock_info as si\n",
        "from collections import deque\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db8fcc8a",
      "metadata": {
        "id": "db8fcc8a"
      },
      "outputs": [],
      "source": [
        "# set seed, so we can get the same results after rerunning several times\n",
        "np.random.seed(314)\n",
        "tf.random.set_seed(314)\n",
        "random.seed(314)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1d495f",
      "metadata": {
        "id": "1b1d495f"
      },
      "outputs": [],
      "source": [
        "SCALE = True\n",
        "scale_str = f\"sc-{int(SCALE)}\"\n",
        "SHUFFLE = True\n",
        "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
        "SPLIT_BY_DATE = False\n",
        "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
        "TEST_SIZE = 0.2\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "### model parameters\n",
        "\n",
        "layers = 2\n",
        "cell = LSTM\n",
        "units = 256\n",
        "dropout = 0.3\n",
        "bidirectional = False\n",
        "steps = 15\n",
        "lookup_step = 3\n",
        "\n",
        "### training parameters\n",
        "\n",
        "loss = \"mae\"\n",
        "optimizer = \"adamax\"\n",
        "batc = 10\n",
        "epochs = 30\n",
        "\n",
        "ticker = \"NDAQ\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
        "{loss}-{optimizer}-{cell.__name__}-seq-{steps}-step-{lookup_step}-layers-{layers}-units-{units}\"\n",
        "if bidirectional:\n",
        "    model_name += \"-b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "131e5652",
      "metadata": {
        "id": "131e5652"
      },
      "outputs": [],
      "source": [
        "def shuffle_in_unison(a, b):\n",
        "    state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(state)\n",
        "    np.random.shuffle(b)\n",
        "\n",
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
        "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "    if isinstance(ticker, str):\n",
        "        df = si.get_data(ticker)\n",
        "    elif isinstance(ticker, pd.DataFrame):\n",
        "        df = ticker\n",
        "    else:\n",
        "        raise TypeError(\"ticker can be a str\")\n",
        "\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns, f\"'{col}' not in DF.\"\n",
        "\n",
        "    # add date as a column\n",
        "    if \"date\" not in df.columns:\n",
        "        df[\"date\"] = df.index\n",
        "\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    \n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "\n",
        "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "\n",
        "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
        "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    \n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    if split_by_date:\n",
        "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
        "        train_samples = int((1 - test_size) * len(X))\n",
        "        result[\"X_train\"] = X[:train_samples]\n",
        "        result[\"y_train\"] = y[:train_samples]\n",
        "        result[\"X_test\"]  = X[train_samples:]\n",
        "        result[\"y_test\"]  = y[train_samples:]\n",
        "        if shuffle:\n",
        "            # shuffle the datasets for training (if shuffle parameter is set)\n",
        "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
        "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
        "    else:    \n",
        "        # split the dataset randomly\n",
        "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
        "                                                                                test_size=test_size, shuffle=shuffle)\n",
        "\n",
        "    # get the list of test set dates\n",
        "    dates = result[\"X_test\"][:, -1, -1]\n",
        "    # retrieve test features from the original dataframe\n",
        "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
        "    # remove duplicated dates in the testing dataframe\n",
        "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
        "    # remove dates from the training/testing sets & convert to float32\n",
        "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a5dcf2",
      "metadata": {
        "id": "50a5dcf2"
      },
      "outputs": [],
      "source": [
        "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d47850",
      "metadata": {
        "id": "95d47850"
      },
      "outputs": [],
      "source": [
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")\n",
        "\n",
        "# load the data\n",
        "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
        "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
        "                feature_columns=FEATURE_COLUMNS)\n",
        "\n",
        "# save the dataframe\n",
        "data[\"df\"].to_csv(ticker_data_filename)\n",
        "\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3268e394",
      "metadata": {
        "scrolled": false,
        "id": "3268e394",
        "outputId": "3d6cc79c-f1d7-4488-a7b9-424684c1c705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 5.8588e-04 - mean_absolute_error: 0.0188\n",
            "Epoch 1: val_loss improved from inf to 0.00010, saving model to results\\2022-06-28_NDAQ-sh-1-sc-1-sbd-0-mae-adamax-LSTM-seq-15-step-3-layers-2-units-256.h5\n",
            "402/402 [==============================] - 25s 46ms/step - loss: 5.8560e-04 - mean_absolute_error: 0.0187 - val_loss: 9.7637e-05 - val_mean_absolute_error: 0.0093\n",
            "Epoch 2/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 3.2193e-04 - mean_absolute_error: 0.0158\n",
            "Epoch 2: val_loss did not improve from 0.00010\n",
            "402/402 [==============================] - 17s 42ms/step - loss: 3.2193e-04 - mean_absolute_error: 0.0158 - val_loss: 2.6044e-04 - val_mean_absolute_error: 0.0157\n",
            "Epoch 3/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 3.6629e-04 - mean_absolute_error: 0.0166\n",
            "Epoch 3: val_loss did not improve from 0.00010\n",
            "402/402 [==============================] - 15s 36ms/step - loss: 3.6740e-04 - mean_absolute_error: 0.0166 - val_loss: 1.1697e-04 - val_mean_absolute_error: 0.0091\n",
            "Epoch 4/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 3.3676e-04 - mean_absolute_error: 0.0160\n",
            "Epoch 4: val_loss did not improve from 0.00010\n",
            "402/402 [==============================] - 20s 50ms/step - loss: 3.3672e-04 - mean_absolute_error: 0.0160 - val_loss: 2.4278e-04 - val_mean_absolute_error: 0.0141\n",
            "Epoch 5/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 3.3886e-04 - mean_absolute_error: 0.0159\n",
            "Epoch 5: val_loss improved from 0.00010 to 0.00007, saving model to results\\2022-06-28_NDAQ-sh-1-sc-1-sbd-0-mae-adamax-LSTM-seq-15-step-3-layers-2-units-256.h5\n",
            "402/402 [==============================] - 18s 44ms/step - loss: 3.3886e-04 - mean_absolute_error: 0.0159 - val_loss: 7.3942e-05 - val_mean_absolute_error: 0.0083\n",
            "Epoch 6/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 3.3465e-04 - mean_absolute_error: 0.0159\n",
            "Epoch 6: val_loss did not improve from 0.00007\n",
            "402/402 [==============================] - 17s 41ms/step - loss: 3.3465e-04 - mean_absolute_error: 0.0159 - val_loss: 1.4616e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 7/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 2.8282e-04 - mean_absolute_error: 0.0145\n",
            "Epoch 7: val_loss did not improve from 0.00007\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 2.8282e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3386e-04 - val_mean_absolute_error: 0.0110\n",
            "Epoch 8/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 2.7677e-04 - mean_absolute_error: 0.0143\n",
            "Epoch 8: val_loss did not improve from 0.00007\n",
            "402/402 [==============================] - 17s 42ms/step - loss: 2.7661e-04 - mean_absolute_error: 0.0143 - val_loss: 7.7562e-05 - val_mean_absolute_error: 0.0078\n",
            "Epoch 9/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 3.0581e-04 - mean_absolute_error: 0.0148\n",
            "Epoch 9: val_loss did not improve from 0.00007\n",
            "402/402 [==============================] - 19s 47ms/step - loss: 3.0562e-04 - mean_absolute_error: 0.0148 - val_loss: 8.3916e-05 - val_mean_absolute_error: 0.0092\n",
            "Epoch 10/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 2.5579e-04 - mean_absolute_error: 0.0138\n",
            "Epoch 10: val_loss improved from 0.00007 to 0.00006, saving model to results\\2022-06-28_NDAQ-sh-1-sc-1-sbd-0-mae-adamax-LSTM-seq-15-step-3-layers-2-units-256.h5\n",
            "402/402 [==============================] - 16s 41ms/step - loss: 2.5579e-04 - mean_absolute_error: 0.0138 - val_loss: 5.9382e-05 - val_mean_absolute_error: 0.0070\n",
            "Epoch 11/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 2.8518e-04 - mean_absolute_error: 0.0143\n",
            "Epoch 11: val_loss did not improve from 0.00006\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 2.8518e-04 - mean_absolute_error: 0.0143 - val_loss: 5.9799e-05 - val_mean_absolute_error: 0.0072\n",
            "Epoch 12/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 2.3910e-04 - mean_absolute_error: 0.0134\n",
            "Epoch 12: val_loss did not improve from 0.00006\n",
            "402/402 [==============================] - 16s 39ms/step - loss: 2.3910e-04 - mean_absolute_error: 0.0134 - val_loss: 1.1595e-04 - val_mean_absolute_error: 0.0107\n",
            "Epoch 13/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 2.1615e-04 - mean_absolute_error: 0.0130\n",
            "Epoch 13: val_loss did not improve from 0.00006\n",
            "402/402 [==============================] - 16s 39ms/step - loss: 2.1615e-04 - mean_absolute_error: 0.0130 - val_loss: 7.1419e-05 - val_mean_absolute_error: 0.0074\n",
            "Epoch 14/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 2.3474e-04 - mean_absolute_error: 0.0132\n",
            "Epoch 14: val_loss did not improve from 0.00006\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 2.3474e-04 - mean_absolute_error: 0.0132 - val_loss: 6.7781e-05 - val_mean_absolute_error: 0.0076\n",
            "Epoch 15/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 2.0663e-04 - mean_absolute_error: 0.0127\n",
            "Epoch 15: val_loss did not improve from 0.00006\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 2.0663e-04 - mean_absolute_error: 0.0127 - val_loss: 1.5540e-04 - val_mean_absolute_error: 0.0126\n",
            "Epoch 16/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 2.2241e-04 - mean_absolute_error: 0.0129\n",
            "Epoch 16: val_loss improved from 0.00006 to 0.00005, saving model to results\\2022-06-28_NDAQ-sh-1-sc-1-sbd-0-mae-adamax-LSTM-seq-15-step-3-layers-2-units-256.h5\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 2.2226e-04 - mean_absolute_error: 0.0129 - val_loss: 5.4611e-05 - val_mean_absolute_error: 0.0067\n",
            "Epoch 17/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 2.2771e-04 - mean_absolute_error: 0.0131\n",
            "Epoch 17: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 2.2771e-04 - mean_absolute_error: 0.0131 - val_loss: 2.0013e-04 - val_mean_absolute_error: 0.0131\n",
            "Epoch 18/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 1.9951e-04 - mean_absolute_error: 0.0124\n",
            "Epoch 18: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 16s 39ms/step - loss: 1.9951e-04 - mean_absolute_error: 0.0124 - val_loss: 5.9899e-05 - val_mean_absolute_error: 0.0068\n",
            "Epoch 19/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 2.0640e-04 - mean_absolute_error: 0.0123\n",
            "Epoch 19: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 17s 43ms/step - loss: 2.0631e-04 - mean_absolute_error: 0.0123 - val_loss: 1.0701e-04 - val_mean_absolute_error: 0.0110\n",
            "Epoch 20/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 1.9263e-04 - mean_absolute_error: 0.0121\n",
            "Epoch 20: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 17s 42ms/step - loss: 1.9263e-04 - mean_absolute_error: 0.0121 - val_loss: 6.7459e-05 - val_mean_absolute_error: 0.0076\n",
            "Epoch 21/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 1.8972e-04 - mean_absolute_error: 0.0120\n",
            "Epoch 21: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 17s 43ms/step - loss: 1.8972e-04 - mean_absolute_error: 0.0120 - val_loss: 6.6495e-05 - val_mean_absolute_error: 0.0076\n",
            "Epoch 22/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 1.9722e-04 - mean_absolute_error: 0.0122\n",
            "Epoch 22: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 17s 42ms/step - loss: 1.9737e-04 - mean_absolute_error: 0.0122 - val_loss: 8.9601e-05 - val_mean_absolute_error: 0.0083\n",
            "Epoch 23/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 2.0249e-04 - mean_absolute_error: 0.0125\n",
            "Epoch 23: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 17s 42ms/step - loss: 2.0549e-04 - mean_absolute_error: 0.0126 - val_loss: 1.1793e-04 - val_mean_absolute_error: 0.0104\n",
            "Epoch 24/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 1.8067e-04 - mean_absolute_error: 0.0120\n",
            "Epoch 24: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 1.8055e-04 - mean_absolute_error: 0.0120 - val_loss: 7.9102e-05 - val_mean_absolute_error: 0.0098\n",
            "Epoch 25/30\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "402/402 [==============================] - ETA: 0s - loss: 1.8110e-04 - mean_absolute_error: 0.0120\n",
            "Epoch 25: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 1.8110e-04 - mean_absolute_error: 0.0120 - val_loss: 6.0787e-05 - val_mean_absolute_error: 0.0082\n",
            "Epoch 26/30\n",
            "401/402 [============================>.] - ETA: 0s - loss: 1.8525e-04 - mean_absolute_error: 0.0120\n",
            "Epoch 26: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 1.8589e-04 - mean_absolute_error: 0.0120 - val_loss: 6.2829e-05 - val_mean_absolute_error: 0.0081\n",
            "Epoch 27/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 1.7000e-04 - mean_absolute_error: 0.0119\n",
            "Epoch 27: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 16s 41ms/step - loss: 1.7000e-04 - mean_absolute_error: 0.0119 - val_loss: 5.8160e-05 - val_mean_absolute_error: 0.0067\n",
            "Epoch 28/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 1.7587e-04 - mean_absolute_error: 0.0117\n",
            "Epoch 28: val_loss improved from 0.00005 to 0.00005, saving model to results\\2022-06-28_NDAQ-sh-1-sc-1-sbd-0-mae-adamax-LSTM-seq-15-step-3-layers-2-units-256.h5\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 1.7587e-04 - mean_absolute_error: 0.0117 - val_loss: 5.3351e-05 - val_mean_absolute_error: 0.0068\n",
            "Epoch 29/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 1.6063e-04 - mean_absolute_error: 0.0116\n",
            "Epoch 29: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 1.6063e-04 - mean_absolute_error: 0.0116 - val_loss: 1.8470e-04 - val_mean_absolute_error: 0.0126\n",
            "Epoch 30/30\n",
            "402/402 [==============================] - ETA: 0s - loss: 1.9498e-04 - mean_absolute_error: 0.0121\n",
            "Epoch 30: val_loss did not improve from 0.00005\n",
            "402/402 [==============================] - 16s 40ms/step - loss: 1.9498e-04 - mean_absolute_error: 0.0121 - val_loss: 8.1429e-05 - val_mean_absolute_error: 0.0080\n"
          ]
        }
      ],
      "source": [
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "# train the model and save the weights whenever we see \n",
        "# a new optimal model using ModelCheckpoint\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "625bf1e2",
      "metadata": {
        "id": "625bf1e2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graph(test_df):\n",
        "    \"\"\"\n",
        "    This function plots true close price along with predicted close price\n",
        "    with blue and red colors respectively\n",
        "    \"\"\"\n",
        "    plt.plot(test_df[f'true_adjclose_{lookup_step}'], c='b')\n",
        "    plt.plot(test_df[f'adjclose_{lookup_step}'], c='g')\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend([\"real Price\", \"Estimated Price\"])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50422aaf",
      "metadata": {
        "id": "50422aaf"
      },
      "outputs": [],
      "source": [
        "def get_final_df(model, data):\n",
        "    \"\"\"\n",
        "    This function takes the `model` and `data` dict to \n",
        "    construct a final dataframe that includes the features along \n",
        "    with true and predicted prices of the testing dataset\n",
        "    \"\"\"\n",
        "    # if predicted future price is higher than the current, \n",
        "    # then calculate the true future price minus the current price, to get the buy profit\n",
        "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
        "    # if the predicted future price is lower than the current price,\n",
        "    # then subtract the true future price from the current price\n",
        "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_test = data[\"y_test\"]\n",
        "    # perform prediction and get prices\n",
        "    y_pred = model.predict(X_test)\n",
        "    if SCALE:\n",
        "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    test_df = data[\"test_df\"]\n",
        "    # add predicted future prices to the dataframe\n",
        "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
        "    # add true future prices to the dataframe\n",
        "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
        "    # sort the dataframe by date\n",
        "    test_df.sort_index(inplace=True)\n",
        "    final_df = test_df\n",
        "    # add the buy profit column\n",
        "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
        "                                    final_df[\"adjclose\"], \n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    # add the sell profit column\n",
        "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
        "                                    final_df[\"adjclose\"], \n",
        "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
        "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
        "                                    # since we don't have profit for last sequence, add 0's\n",
        "                                    )\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90639344",
      "metadata": {
        "id": "90639344"
      },
      "outputs": [],
      "source": [
        "def predict(model, data):\n",
        "    # retrieve the last sequence from data\n",
        "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
        "    # expand dimension\n",
        "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "    # get the prediction (scaled from 0 to 1)\n",
        "    prediction = model.predict(last_sequence)\n",
        "    # get the price (by inverting the scaling)\n",
        "    if SCALE:\n",
        "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
        "    else:\n",
        "        predicted_price = prediction[0][0]\n",
        "    return predicted_price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407e8794",
      "metadata": {
        "id": "407e8794"
      },
      "outputs": [],
      "source": [
        "# load optimal model weights from results folder\n",
        "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
        "model.load_weights(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986f6837",
      "metadata": {
        "id": "986f6837"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
        "# calculate the mean absolute error (inverse scaling)\n",
        "if SCALE:\n",
        "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
        "else:\n",
        "    mean_absolute_error = mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b3e032",
      "metadata": {
        "id": "61b3e032"
      },
      "outputs": [],
      "source": [
        "# get the final dataframe for the testing set\n",
        "final_df = get_final_df(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a69f7b7",
      "metadata": {
        "id": "6a69f7b7"
      },
      "outputs": [],
      "source": [
        "# predict the future price\n",
        "future_price = predict(model, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53ca69ff",
      "metadata": {
        "id": "53ca69ff"
      },
      "outputs": [],
      "source": [
        "# we calculate the accuracy by counting the number of positive profits\n",
        "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
        "# calculating total buy & sell profit\n",
        "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
        "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
        "# total profit by adding sell & buy together\n",
        "total_profit = total_buy_profit + total_sell_profit\n",
        "# dividing total profit by number of testing samples (number of trades)\n",
        "profit_per_trade = total_profit / len(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec74e44c",
      "metadata": {
        "id": "ec74e44c",
        "outputId": "ddb5abae-16b8-45c7-b642-529d7a565ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Future price after 3 days is 157.60USD\n",
            "huber_loss loss: 5.335121022653766e-05\n",
            "Mean Absolute Error: 5.833778179176693\n",
            "Accuracy score: 0.5268924302788844\n",
            "Total B profit: 137.11113214492784\n",
            "Total S profit: 32.72024059295659\n",
            "Total P&L: 169.83137273788444\n",
            "Profit: 0.16915475372299246\n"
          ]
        }
      ],
      "source": [
        "# printing metrics\n",
        "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}USD\")\n",
        "print(f\"{LOSS} loss:\", loss)\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
        "print(\"Accuracy score:\", accuracy_score)\n",
        "print(\"Total B profit:\", total_buy_profit)\n",
        "print(\"Total S profit:\", total_sell_profit)\n",
        "print(\"Total P&L:\", total_profit)\n",
        "print(\"Profit:\", profit_per_trade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84c45d99",
      "metadata": {
        "id": "84c45d99",
        "outputId": "faafd739-7234-4535-cb35-98ee065b24c0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA67klEQVR4nO3dd3gVRdvA4d+kVwgkBAIhhC4BQoQgFkBEREEEeS3Yu4iFV31FBf1EbIhdELEXUFSw0BSlI016h9AhJJRU0gs5OfP9sYeTRNI5JeW5r+tc2Z2d3X0mgTzZ3dkZpbVGCCGEAHBxdgBCCCFqDkkKQgghrCQpCCGEsJKkIIQQwkqSghBCCCs3ZwdwIYKCgnR4eLizwxBCiFply5YtyVrrJqVtq9VJITw8nM2bNzs7DCGEqFWUUrFlbZPbR0IIIawkKQghhLCSpCCEEMKqVj9TKE1BQQHx8fHk5eU5OxRRTV5eXoSGhuLu7u7sUISod+pcUoiPj8ff35/w8HCUUs4OR1SR1pqUlBTi4+Np3bq1s8MRot6pc7eP8vLyCAwMlIRQSymlCAwMlCs9IZykziUFQBJCLSc/PyGcp04mBSGEqI1iYmDlSufGIEmhBgoPDyc5ObnU8q5duxIZGcnAgQM5ffp0qfsPHjyYtLQ0O0cphLC1iEF/c9X8MNLz0p0WgyQFO9JaYzabbXrMFStWsHPnTqKjo5k4cWKp51u4cCEBAQE2Pa8QwgF6vwUN4/hpx1z27HFOCJIUbOzYsWN07NiRe+65hy5duhAXF8c777xDz549iYyM5OWXX7bWvfHGG+nRowedO3fm888/r9J5+vbty6FDh0o9X/ErjRkzZhAZGUm3bt24++67AUhKSuKmm26iZ8+e9OzZk7Vr19ruGyCEqL6ESACen/kdXX5RLNj/u8NDqHNdUot76inYvt22x4yKgg8/LL/OwYMHmT59OpdeeimLFy/m4MGDbNy4Ea01Q4cOZdWqVfTt25evv/6axo0bk5ubS8+ePbnpppsIDAysVBy///47Xbt2Pe98xe3Zs4fXX3+ddevWERQURGpqKgBPPvkkTz/9NL179+b48eNce+21xMTEVPVbIYSwNc8MANIDlwHw0+5Z3NBxiENDqNNJwVlatWpl/QW9ePFiFi9ezMUXXwxAVlYWBw8epG/fvkyZMoU5c+YAEBcXx8GDBytMCldddRWurq5ERkby+uuvk5aWVuJ8xS1fvpxbbrmFoKAgABo3bgzA0qVL2bt3r7VeRkYGWVlZ+Pn5XXjjhRDV53WmxKqb8nB4CHU6KVT0F729+Pr6Wpe11owbN45HHnmkRJ2VK1eydOlS/vnnH3x8fOjXr1+l+uavWLHC+kseIC0trcT5KsNsNrN+/Xq8vLyqtJ8Qws48skusuuHp8BDkmYKdXXvttXz99ddkZWUBcOLECRITE0lPT6dRo0b4+Piwb98+1q9fb/Nz9+/fn59//pmUlBQA6+2jgQMH8tFHH1nrbbf1PTYhRJWZzYB7yaTgiuOvFOyWFJRSLZVSK5RSe5VSe5RST1rKGyulliilDlq+NrKUK6XUFKXUIaXUTqVUd3vF5kgDBw7kjjvu4LLLLqNr167cfPPNZGZmct1112EymejUqRNjx44t9fbPhercuTMvvvgiV155Jd26deN///sfAFOmTGHz5s1ERkYSERHBp59+avNzCyGqJj8f40ohvpe1zBlJQWmt7XNgpUKAEK31VqWUP7AFuBG4D0jVWk9SSo0FGmmtn1dKDQZGA4OBXsBkrXWv0o9uiI6O1v+eZCcmJoZOnTrZvD3CseTnKOqb2NMZhH/WEE5Eg18CNIxjVOdxfHLzxIp3riKl1BatdXRp2+x2paC1PqW13mpZzgRigBbAMGC6pdp0jESBpXyGNqwHAiyJRQgh6rzZe38yFpruggSjZ2FuQb7D43DIMwWlVDhwMbABaKq1PmXZdBpoalluAcQV2y3eUiaEEHVeSpKl38/nm2H7/QDkFjh+YEi7JwWllB/wK/CU1jqj+DZt3Luq0v0rpdRIpdRmpdTmpKQkG0YqhBDOcyDhOGjF4F4dYO/NkB5a95KCUsodIyHM1Fr/ZilOOHdbyPI10VJ+AmhZbPdQS1kJWuvPtdbRWuvoJk2a2C94IYRwoBPZsZAZwtTJlofLJi9yC3IdHoc9ex8p4CsgRmv9frFN84F7Lcv3AvOKld9j6YV0KZBe7DaTEELUaadyjqMywwgPhxUrAJMXeSbHXynY8+W1K4C7gV1Kqe2WsheAScBspdSDQCxwq2XbQoyeR4eAHOB+O8YmhBA1SmphLH6maJQCDw/A5E1+YR26faS1XqO1VlrrSK11lOWzUGudorW+WmvdXms9QGudaqmvtdaPa63baq27aq03V3SOmsrV1ZWoqCjrZ9KkSWXWnTt3bokhJ8aPH8/SpUsvOIa0tDSmTZtW5f0mTJjAu+++W2p5ixYtiIqKokuXLsyfP7/U/T/99FNmzJhR5fMKUZ+ZtZlsz8M0dg0DziUFL/JMjr99VKeHuXAWb2/vSr8lPHfuXIYMGUJERAQAr776qk1iOJcUHnvsMZscD+Dpp59mzJgxxMTE0KdPHxITE3FxKfq7wmQyMWrUKJudT4j64O+/4dXpa6EVBPo1BMDTEyMpFGY6PB4Z5sKBxo4dS0REBJGRkYwZM4Z169Yxf/58nn32WaKiojh8+DD33Xcfv/zyC2BMqjNu3DiioqKIjo5m69atXHvttbRt29b6FnJWVhZXX3013bt3p2vXrsybN896rsOHDxMVFcWzzz4LUOYQ3m+88QYdOnSgd+/e7N+/v8J2dOrUCTc3N5KTk+nXrx9PPfUU0dHRTJ48ucSVxqFDhxgwYADdunWje/fuHD58uNw4hKiPRo+G5UdWANDNdQQATZsCBd7knK1bzxSc7qm/nmL76e02PWZUsyg+vO7Dcuvk5uYSFRVlXR83bhwDBgxgzpw57Nu3D6UUaWlpBAQEMHToUIYMGcLNN99c6rHCwsLYvn07Tz/9NPfddx9r164lLy+PLl26MGrUKLy8vJgzZw4NGjQgOTmZSy+9lKFDhzJp0iR2795tvWIpawhvX19ffvrpJ7Zv347JZKJ79+706NGj3PZt2LABFxcXzvX+Onv2LOfeLJ8wYYK13p133snYsWMZPnw4eXl5mM3mcocSF6I+at+hkF0tv4LDAwhv1w6AoCBwMTun91GdTgrOUtrtI5PJhJeXFw8++CBDhgxhyJDKjZE+dOhQALp27UpWVhb+/v74+/vj6elpHSH1hRdeYNWqVbi4uHDixAkSEhLOO05ZQ3hnZmYyfPhwfHx8SpyvNB988AHff/89/v7+zJo1C6ODGYwYMeK8upmZmZw4cYLhw4cDWEdkLW8ocSHqG7MZ/o5fDF2Pw+J3uf5Jo9zFBTxcvDhrlisFm6roL3pHcnNzY+PGjSxbtoxffvmFqVOnsnz58gr38/Q0hs51cXGxLp9bN5lMzJw5k6SkJLZs2YK7uzvh4eGlDsFd1hDeH1ZhfPFzzxT+rSpDd5cVhxD10bFjkNL8RzjrA/uHUfy/rhvemFQdek9BlJSVlUV6ejqDBw/mgw8+YMeOHQD4+/uTmVn9h0np6ekEBwfj7u7OihUriI2NLfW4ZQ3h3bdvX+bOnUtubi6ZmZksWLDgAlpZxN/fn9DQUObOnQtAfn4+OTk5ZcYhRH205dgB6PoD/QMe4r67PSh+59ZN+2JS2WXvbCd1+krBWf79TOG6667jySefZNiwYeTl5aG15v33jff5brvtNh5++GGmTJlifcBcFXfeeSc33HADXbt2JTo6mosuugiAwMBArrjiCrp06cKgQYN45513iImJ4bLLLgPAz8+P77//nu7duzNixAi6detGcHAwPXv2vPBvgMV3333HI488wvjx43F3d+fnn39m4MCBpcYRHBxss/MKUVusjVsLLoU80uNRbn225DZ37YfJJRuzNuOiHPf3u92GznYEGTq77pKfo6gPrn55EstdxnH8kWxaNvMpsS10xDuciHiOzHGZ+HnYdqpcpwydLYQQonw7jyTgavI7LyEAeOIPQNbZLIfGJElBCCGcIC4OknMTCHBvWup2T2VcHWTmO/YFtjqZFGrzLTEhPz9RP8TGAr4JBPuW/jzN08W4esi1DHWRnpfukP8bdS4peHl5kZKSIr9YaimtNSkpKdb3GoSoq3JzAb8EgrxLv1LwcjP+D+SZ8jiZnkjAWwG8vfZtu8dV53ofhYaGEh8fj0zAU3t5eXkRGhrq7DCEsKucHMA3kWCf3qVu93Q1kkJuQS7PvnYS/OG1Va/xfO/n7RpXnUsK7u7utG7d2tlhCCFEuTKzTeCTTFO/0q8UfNy9AeNKYc2WAugH2QX2f2+hzt0+EkKI2iApOxmUJsS/jNtH7saVwnUzr+N4crLD4pKkIIQQTpCQbYxR1rxh6Q+a/TyLPVfzPX88M3uRpCCEEA525Ai89UkcAG2DWpZaJ8ivUdFK2BpHhAVIUhBCCIf76IszcNV4ADoEh5dap3mDZrDcMulWhz+s5Sazya6xSVIQQggHW548E0K20dKnA838S7995OsLxFqGlHcvGi0135Rv19gkKQghhIOl5aUDcPDpndZ5SUqV2/i8ojyTfedYkKQghBAOln02G6Vd8XD1KLPOHXcgSUEIIeqDnIJs3LVfuVcJDRvC9M+KPWzOa2B8kaQghBB1h9aQZ87GQ1U8Y2FwI2/rsio0xkKSpCCEEHVIejpo9yy8XCpOCu3bF11JKDfjAXN+oTxoFkKIOiMlBfBNJMAjqMK6bdoA+28wVtyMHkhypSCEEHVIcjIQcIwWvhWP0aYUsP4pAMyuRjKQpCCEEHVIYrIJGh4nPCC8UvVnftAFgIvSnwAkKQghRJ1yKOEEuBTSLii8UvXvGBaMflnTrfAhQJKCEELUKYdTjwEQEVK1If4b+p2bX0GSghBC1BnHM44C0CU0vEr7NfI3kkJmriQFIYSoE46nH2eB6/0AtAoofXTUspxLCmlZkhSEEKJO+OvQX9ZlTzfPKu3b2JIU0rMlKQghRJ2w8+Q+YyH+kirvG9jQSAoZOfls3QonT9oysiKSFIQQwkGmzz0OaWHwzeoq7xsYYAyel5mbx+WXw5Qpto7OIElBCCEcJIvTkNoOCsseHbUsDRsqMHmSmZNHfj54eVW8T3VIUhBCCAcwmwG/05DdlK+/rvr+DRoAJi/rMwVv7/LrV5ckBSGEcIDYWMDvND06NuO++6q+f8OGgMmLDEuXVHtdKbjZ57BCCCGK27o7Czyy6dmuGeVNtlYWf3/A5EWWnZOCXCkIIYQD7D95GoD2zZpVa38PD1CFXuw/XEtvHymlvlZKJSqldhcrm6CUOqGU2m75DC62bZxS6pBSar9S6lp7xSWEEM4Qm2wkhXbVTAoArtoLusyG7l/UyiuFb4HrSin/QGsdZfksBFBKRQC3AZ0t+0xTSrnaMTYhhHCo+AzjxYLwoOonBTcsmWDoSArd020R1nnslhS01quA1EpWHwb8pLXO11ofBQ4BVX+7QwghaqjTOXEAhDUMq/YxlLnoLegk874Ljqk0znim8IRSaqfl9tK5WalbAHHF6sRbys6jlBqplNqslNqclJRk71iFEMImUgricDH50dCzYbWPUagLrMvrkn+3RVjncXRS+ARoC0QBp4D3qnoArfXnWutorXV0kyZNbByeEELYR6bLcbzzw1DV6XpkUeieZl3elbnKBlGdz6FJQWudoLUu1FqbgS8oukV0Aig+ZGCopUwIIeqEXM/j+JiqNjLqvxV6nAGgwdmOLHroJ1uEdR6HJgWlVEix1eHAuZ5J84HblFKeSqnWQHtgoyNjE0IIezrrFYe/ufrPEwDQxpd5d/xKaEBI+XWryW4vrymlfgT6AUFKqXjgZaCfUioKo2nHgEcAtNZ7lFKzgb2ACXhca11or9iEEMKRTiXnUuiVSIOcC0wKq/4Pek4jIqStbQIrhd2Sgtb69lKKvyqn/hvAG/aKRwghnOXnVdsBCPQJvLADbXocNj1Oow8vOKQyyRvNQghhZx9tnArAiP6dbHI8d3ebHKZUkhSEEMKO3vzlTw55/wAmTwZ16ufscCokSUEIIexk1+m9vLDHMpqPWz6hoRd2vDZtLjymisgoqUIIYScj5z1u0+Nt2QLp9hndwkqSghBC2MnelB02PV5AgPGxJ7l9JIQQdpKRm21dvvfsBidGUnmSFIQQwl6ymxpff5xH49zaMcanJAUhhLAXZYZt98P+oeTlOTuYypGkIIQQdpCSosEnGbKNgTsffdTJAVWSJAUhhLCDzTuzwC0fcoJYtgy6dnV2RJUjSUEIIexg095kYyGndg3xL0lBCCHs4EC8MQlY/0uD6NPHycFUgSQFIYS4QMfSjnH1jKs5kVE0DUx8qnGlMPHFJnYdq8jWJCkIIcQFeHnFy7Se3JrlR5ez4tgKa/npTONKIcgnyFmhVYskBSGEqKZ9yft4ddWr1nWT2WRdTs5JBKCJb+16piDDXAghRDXtSzhaYv1M7hkOpx7hxy2/k6z2420OpoFnAydFVz2SFIQQopq27c4yFqbtRD3WjaMJqXT85RoKGxyBFp0IcYtwboDVILePhBCimuISLUkhvwHuhQHM/esMhb5xRlmTGFr5dXBecNUkSUEIIarpdKqRFJTJD5XXiFP+v4NrgXV7aEBTZ4VWbZIUhBCimpLSjaQw7hk/8tMaY/KLLbH9v9FjnBHWBZGkIIQQ1ZR5NhPMbvTr7QFKl9z4Xjztw2rXQ2aQpCCEENWWU5CFa6EfrVsrCDxQtOHHuXw0sQUNGzovtuqS3kdCCFFNueZM3M1+NGsGFFpeW56YCWf9uOUWp4ZWbXKlIIQQ1ZTlEo+vuTl+fsC3K2HOt7w+3g8XF2jc2NnRVY8kBSGEqAazGfI8Y2niEQ5A5yZdYce9vPgiFBZSq8Y7Kk6SghBCVFGhuZA3l0+BwIO08G0FwKZNEBfn5MBsoFJJQSnVQSm1TCm127IeqZT6P/uGJoQQNdOMv1fzf2ufBKBlo2YAeHtDaKgzo7KNyl4pfAGMAwoAtNY7gdvsFZQQQtQUheZC1sevL1H2+exj1uVuLTo5OCL7qmxS8NFab/xXmanUmkIIUYe8u3Q6l311Gb/u/dVatj7G8pLalIPc3O06J0VmH5VNCslKqbaABlBK3QycsltUQghRQ7z0ag4A3+/6HjAeMLsGHoPMEEhtR7NmyonR2V5l31N4HPgcuEgpdQI4Ctxlt6iEEKKGKDhr/NJfuG8JWmv271cUBu6GJOO2kVsde9urUlcKWusjWusBQBPgIq11b631MbtGJoQQNYFHNgBnySY+I551600QvJub+0Sxe7eTY7ODyvY+mqiUCtBaZ2utM5VSjZRSr9s7OCGEcDr3bOtiYnYiS7ceBPc8ru/Rjc6dnRiXnVT2mcIgrXXauRWt9RlgsF0iEkKIGiS0TVFSSMpJYv2xHQBcHNLNWSHZVWWTgqtSyvPcilLKG/Asp74QQtQJ+eaipHAiLYnY/O24aHc6NalbXVHPqewjkpnAMqXUN5b1+4Hp9glJCCFqjiyv/XAmHBodY0tMMjp4B628O+Hh6uHs0Oyisg+a3wLeADpZPq9prd+2Z2BCCOFsqw9vIrfZCnxOXg+FbmzcnQTNdnBx87p56wiqMHS21vpP4E87xiKEEDVG9tls+n5/CQDXR3fl58QgdiTuhban6NUqyrnB2VG5VwpKqTWWr5lKqYxin0ylVIZjQhRCCMdbF7fOuvxgrxH40ART6DIAokPr7pVCuUlBa93b8tVfa92g2Mdfa13uPHNKqa+VUonnBtGzlDVWSi1RSh20fG1kKVdKqSlKqUNKqZ1Kqe62aJwQQlTHiYwT3DTLMkvOxEwiOwTQrEET8DTmZG7dqLUTo7OvCp8pKKVclVL7qnHsb4F/DwoyFlimtW4PLLOsAwwC2ls+I4FPqnE+IYSwie92fkdmQTocGsi0D/0ICYEOLZpYt/t5+DkxOvuqMClorQuB/UqpsKocWGu9Ckj9V/EwinotTQduLFY+QxvWAwFKqZCqnE8IIarjm23fcDj1cImyTQeOAxC6egGjRhllbUPqR1Ko7IPmRsAepdRGwNppV2s9tIrna6q1PjeQ3mmgqWW5BVB8eop4S9l5g+4ppUZiXE0QFlalPCWEECUkp5p4YP4DeCo/8sZnsnTfei5u2ZFVO45DfhRffuaBsox3F+xblBS83bydFLH9VTYpvGTrE2uttVJKV2O/zzEG5yM6OrrK+wshxDlbdmcCkK+zePDjL/k6+WEanY0kzaRp0ySca68tqtvEpygpKFW3RkYtrtykoJTyAkYB7YBdwFda6wuZRyFBKRWitT5luT2UaCk/AbQsVi/UUiaEEHYTE1/0a+br5IcBOOOxE4KhpU/fEnWbFLtSqMsqeqYwHYjGSAiDgPcu8HzzgXsty/cC84qV32PphXQpkF7sNpMQQtjFrrjYMre1ahBeYr34lUJdVtHtowitdVcApdRXwL9nXyuTUupHoB8QpJSKB14GJgGzlVIPArHArZbqCzEG2DsE5GAMoyGEEHZ1KCkW/EvfdmeHx0qs15crhYqSQsG5Ba21qSr30bTWt5ex6epS6mqMiXyEEMJhVvn/69fOx7vhmudg7fO0vcunxCa5UjB0K/bmsgK8LesK43d5uS+wCSFETWX8LVpSp6DOdMr7g1smQdu2Jbc19m7soMicq9ykoLV2dVQgQghxobae2krrgNY08m5UYd3DiSeNhYVTwKWQob3bMm9v2fVdXerHr8M6NruoEKI+WXJ4Ce/+8y59w/rSL7wfvb/pzcC2A1l016IK9+05dKvxJPNUD4i7nNa9Kj5fRJMI+oT1ufDAazBJCkKIWuu1Va+x+vhqFh9ejLuLO2BcLVRGmudOAN57rhvPjIYmlXhksOexPdWOtbaQpCCEqLVOnLL2haHAbCyHNgitcL+8PGjXOYPDhZ48/bgvzRrDLbfYLcxapbLTcQohRI0Slx7HkbwtRQWrXoSzPpw4k1Tuftk5hXiPvpRDzd5Gu+ajFNxxB7i72zngWkKSghCiVpq1aRm4FsAx483jGWPugnXPkpR3koLCArLOZpXoYZSRn8Fnmz/j66VrIXQDAG4FAc4IvUaTpCCEqJVenPkrZDWF75bAW8ncMfAiAl1bgdL8FvMb/m/6M3P7L9b6d/x6B6P+GMULf000CuZ+Q+CCVU6KvuaSpCCEqHUKCgs4G/47xPYhsrMHQ68JxNUVBl/SCYDvdswE4O75t7LksDFb2h8H/wAgq+kiSAsjPO0+fvukq3MaUIPJg2YhRK0zet6LxkLjQ+zYUVTep1ME3x2BzfHbrGXvrvyEnXGHSuzvmXkRR486ItLaR64UhBC1zu5TMQA8GPZ2ifI2zRtAeksS8uKNgqxgTqfm8NJPP5eot37CZIfEWRtJUhBC1BqnMk9x79x72ZCyCPYN46H+15TYHhICJBm3kCjwhlPdOZ2RRJ5bQlE5EB7UzIFR1y6SFIQQtcYbq99gxo4ZmHQBPjufJjKy5PZmzYDU9gB4mANo4NaEM/mJ6IDDuJ68zFqvoWdDB0Zdu8gzBSFErbFtfUOwDEH07O298Ck5kCmNGoFLWjvMgIurmYbuTcjwNeZbjgzuzrafr+WKq7Lr9MxpF0qSghCi1lj3jwl6G8sT/s/rvO1KgXduW7IB7ZpLoFcT6+Tv9wyKIOz0Vbw20mHh1kpy+0gIUSv8sOsHCFsLQMiCTWXWy45rZyy4nqWxV9GARrf378rcudBVeqGWS5KCEKLGGzRzEHf+dqeRFHbegVdqdJl1nxvZGoDhETeyenHRHAhN/YPsHmddILePhBA13l+H/ipaSexKKfPjWL31hhejzhwhxD8Ev4N/82U+uJysxLjYApArBSFEDTZ141SiP7msRFmnwM7MnVv+fq0btcbLzYu7LhsIs39mtO8a+wVZx6jSpqSrLaKjo/XmzZudHYYQwka01iV6BqlXzu8llPZcBg29/St5PJg7F264AdzkvoiVUmqL1rrUe3DybRJC1AiZ+Zk0f785QzsMY0v8Lpbe/wcUuhsjocb3AhcTuJgqnRDA6I00fLgdg66DJCkIIWqEdXHryDqbxQ+7jcHsbv/hESMhfL8QDg0qqviJkwKsJ+SZghCiRvhp568l1tckLASzKxzvzQ03OCmoekiuFIQQNcKivevg8DXgnQI+yRBwHM605uAef5o3h0sugb59nR1l3SdJQQjhdBn5GZwyxUD8f2DFqwR330Di0EvxP9uRdpZ30Xbvdm6M9YXcPhJC2Ez22Wwy8zPL3H4q8xQvLnuRo2eKJjM4lnaMhpMagjJzZYfuTJsG09+4BBZ+xC2eXzoibFGMJAUhhM20ntyaBpMa8MTCJ0rd/si80UxcM5E2U9qw7dQ2cgpyaDO5jbExuQNvPzyYRx+Fq65SjGj9BI/fK0NcO5q8pyCEsAmzNuP6qqt1fendS7m6zdXW9fiMeFp+0BJS20LjwwDcHHEzi/dsIEPFwasFmE1uyACm9lfeewpypSCEsInZy/cZC7tHADDguwEAbDqxCfWK4pWVrxvbf/kR4i41Fvf+YiSETaPYuF4SQk0gSUEIYRMfrp1mLKycYLxsBpzJPcOc3X8C8OW2zwAIUVHw7d/wxXrrvm28etGzpyOjFWWRpCCEsIkjejkAGxd2pHuCMQdyn2/68O2PGUWV/nmK48fc+WWWB9d0vgQODoKNj/HzC/c6I2RRCumSKoSosuVHlxPiF0KnJp0wazOLDy8mrSAJv6MPET1e8dDgnjyWAHuS9kDrPWB2gVcLGTvWGIPopptg0CBFdPRCUlKgc2dnt0icI0lBCFElqbmpXD3DeID83+hnOJVznJ/3/gzuEN28HUrBLTe78Fj/mXDTncZOLmZOnoTg4KLj+PjA3r1OaIAolyQFIUSVjP5tvHV5yub3Smzre5ExrVlQEPRv3Z/laWHgncrDrd8mJMShYYpqkqQghKiSuet3QENvmPUb3DWoxLY7+nezLvurZvBhLK6u8PpJR0cpqkuSghCi0vYmxpATtAY2jcL7xHXkfngUGh6HU92h9TIiX25hrfvuuxAQAJ9+Cl5ezotZVI0kBSFEpXX+JAKA1gHtWLQDOnQIh7RwvvgC4uOHlajbrh18+63jYxQXRpKCEKJMR88cZemRpUQ2jSQ2PRYKvMA9j5eH3Uvr1jB0KDz3HFxxhbMjFbYiSUEIUaaRvz7D0hNzigrcgX1DiRwahJsbzJvntNCEnTjl5TWl1DGl1C6l1Hal1GZLWWOl1BKl1EHL10bOiE0IUWTpxrjzC9PD6Nbt/GJRNzjzjeartNZRxQZlGgss01q3B5ZZ1oUQTlJQAHilnVf+yq234SJjIdRZNen20TCgn2V5OrASeN5ZwQhR3+05lG6MZrpiAuwfBuErYf2TjM2XUevqMmclBQ0sVkpp4DOt9edAU631Kcv200DT0nZUSo0ERgKEhYU5IlYh6pU5MXM4lHqI/PjOoDSvPdyXl+6KgtNRJCeDh4ezIxT25Kyk0FtrfUIpFQwsUUrtK75Ra60tCeM8lgTyORjzKdg/VCHql//M/g8A3fKfADd3RlzRi0W94cEHITDQycEJu3NKUtBan7B8TVRKzQEuARKUUiFa61NKqRAg0RmxCVHfnM46TZdpXXil3yv4ne1gLd/h9hnsuoO2YT6sXu3EAIVDOTwpKKV8ARetdaZleSDwKjAfuBeYZPkqnd2EsJNtp7axLm4d3UO6szp2DSm5KTzx57+m0DR58WCrSfJQuZ5xxpVCU2COMqZYcgN+0Fr/pZTaBMxWSj0IxAK3OiE2Ieq0lJwUAn0CuX32PexP21204awPeOQYy+mh0DCeiMSX+fJLmSO5vnF4UtBaHwHO6+WstU4Brj5/D1GT5RTksOjQIm686EaUzKVYo32z7RsemP8Av163nv0p+8EVONMaGh2FNeMgpT14ZNPL83582m/gs/GXODtk4QQ1qUuqqIXGLR3HlI1TWPvAWi5vebmzwxHl+ODvLwG46a9LjYTw7QraufXjjodSaH5nAKMecQXgrzMQEHCpEyMVziRJQVRLel46yTnJzD8wHzAmXhE1V6G5kF2n9oFPUVnG7ivw9wUwuhS1DoewMGNkU1F/SVIQ1RL9RTSHUg9Z1yUp1Gx/bN8EPqmw+B0Y8DwXu96Dv697iToDBzopOFGjSL+COuTAAUhJccy5iicEgISsBMecWFRaYnYi/8T9Q0JWAo/88RCYPIgsfADezOS+oE+dHZ6ooeRKoQ7p2BEaN7Z/YsjMPL/suY9X08l8G0P6tLTvyUWl5Bbk0mPaZcTnHOHRiJc4bd5Dp11/sHl1Y776CkaMcHaEoqaSK4VabtkymDy5aD3VAXdx/l5dWLIgryF0XMATix+0/8lFhWbtnoXfRH/ic44A8Mne1yDpIqY9NRh3dxg1ChrJGMSiDJIUarkBA+Cp1w6QW5ALaPA7Regjj6JeUYTe/qZdzrk7dWPRSvwlUGgMhnPCdQ1ZuWftck5ROdnZmru+eB0zhfDr95DvB0B7t6vp18+5sYnaQZJCbdcwFkZ35IVlL0L0ZzCmOSeaG/eLT1z0Aik5tr+XNO5wUdfTG9V08E0CwKRy6friw7yybBJmbbb5eQV8uvlT1CuKZUeWEZt2nKNnjpbY/vGiRZga74bfp7Ho3TshvRUAN3SWV4BE5UhSqO26zQBgbex6uPqF8zb/vv+vMnfNyTWjXlG8vOLlSp/OZDZZly+aZebDFy9iWMeiuXmPNZzBhDXjmL9tbaWPKSrns82f8+gfjwIw4LsBhE9uRZspbYjPiAegoEDz/MrRkNGCNZ/fwsCB8Puj0+iob2TcLYOcGbqoRSQp1HYNTgCw6fQ/4H3GKPt4N/w4F4CDCfFl7vrIGGPfV1e9WunT7U0weh112DOdmL2KVq1g9i2zmdoiHdb9z1pv+IK+vL327aq0RJTj5KlCnpn3Ohy/HA4MLrFt0aFFAPzyz0YIPAR/j+eKi4MAuL5LX/ZNmENQgJfDYxa1kySFWiwvD/A7XbJw3zBI6oznsWGQ14D1e0tPCiczT/J9cNXno/hhiTFezqj/dLGWebh68PhDDeDvklcczy+VOZJs5ZlpS8l2i+Nq36cY4POcUTj7Z8gMYfzSN1ATXLhjhfEW8q9vDnFipKK2k6RQix07BgTvKlE2usV3PPAAzJkDeGWwLHMqR84cOW/f694fU2K90FxITkEOZwvLf1C8JW4PaMWt/S86f2N+A5ig4f2ieX2NB+DiQmgNSxN+wOVsAH+8P5Q5H1zJ5EZmts28GQ4P5GTuUVAa9zMRDHCbwH+uae7skEUtJkmhFtt1IBMaH6H5sWcgoSusHcPL4/z56isIDQVORQGw8cTG8/eNP2wsnDCmyH7+rwn4TvTl8k8HlHvOpTt2Q2pbWgT7lFmne7tQ+GFBmecW5UvMTmTlsZXW9RfePkpysx8JzxqBp5snfn7w3/8qoqKAdc8Ylc60Ie/9PSx5sfLPh4QojSSFWmzlnj0ADO/RBz7ZySu937HOjNWoETB9BQBHU2NL7FdYCHhmwv4biE76CID3Nr0OwJZkYzaVtcfXsip2FYWFxuR28Rnx5BXkQ9ga3JJ6lBrPxIlwww3w558QnHcFaMWq2FUArDm+hgX7F9is7XXV0TNHafZuM66afhXbTm0D4IsjL6K0G/Oeeem8+u8825Vex2exY+w8mfdA2IS80VyLbTl2AJrD6Ns60acJXHtt0bbQUBgxLIBZ2UHsPV3y9tGxWDMEHCXC/XqeubITt+8oedyCwgJ6f9PbWCl0Z1LEMsbu70uEz1Xgf5q7epV80HnOuHFFyw/d1YiJiV1ZeWwVL10Jfb7pA8Dvty5h46yrGTdW4SXPPtmbtJf/W/5/PHPZM3QLuoQ2U9pYtz29YDyDw28ipfmP9OUFuoS1OG//MWNgjEw9ImxIkkItdiTlODSHVgFhdCxl2IJbb4VZf7bhYHLJpDBl9Tfgnsd1vdoQFtywaMOa56H3W+w4ubeozLWAsfv7ArA3ZwVoxehB11UYW9++MPG9gaxs+iHx6Set5UNmXwMKNrzzFn+99FzVGlzHfLJgA49v6od2zcPd1Z0DBy3dfRO6wL4b+fvK1/n71O+Q2pY375GH9sIx5IKzlsrNhSTTEXx1U7zcSv+Tu2lT4EwbYjP+lRSOPQRArw7hxrOH+F64aHfYdyMA7/w+x6iY1xDOhJc86Pb76N4xuML4Bg6EyMIHMGPi3fkLz9u+yFy/f8nl5MCYuRPReX60Mg1k9p7ZbD/7GyE7PuT0y7t4qnvRraIxER9zeY8GToxW1CeSFGqp3buBsDV0alD27FidOgFn2pCYH2t96Wz34aLBkbq2DCcsDI5OWEH6uDNGEgBmJ74CwJ6nNxB9dLZROakT/DmZkc2+qlR8SsEb/+sIZ32Yvv0bo3D7vdYH2zWF1poN8RtIzU3l9l9vJzE70SHnnfbTQXJaLiAodhSxf95kFBZ4seyNJ2naFMY+60Hk3jn0OPUJ7zxybfkHE8KG5PZRLbVy6wkIPMg17UeVWadxYwh2b0Mihbi/5k7jQ4+TeqgtXAfD8n6jU5NOAIS38Abg6/fa8sAhN3A10S33SSKCO/LSg3D76L38ObM9ka+74e9f+Rh7dHeB+Z1Ia77OKDjeGxoUvTeRfTYbXw/fqjfehmZs/Yn7fr+Dhq7BpBcmEuIXwvvXvm/3836693Xw9mTGE48z+MYsAHoEXGskcoyrvB2zbrR7HEL8mySFWmrxgZXQAG7qflW59TqHhnPub9/Udh9DOyCpE79OHn5e3V7RHvDkQeiwgE1rHwNg6FDIuqET1Zl+uVEjICkCmm8BYNwdfXhz+8/W7fuS99Gjeek9mRzlnUU/AJBeaHyXPlj/AT6mUF6//n/l7QaAWZspNJsZ/O3NdGjWkqmDp1RqnupvFm/ksO/3tE54mkF9mvHblxDv/Tv39Ot9YY0RwgYkKdRCK4+uYWmDu3Ax+RIVEllu3eiWkaw4t7LxcfDM4Mfn78fV9fy6ERGwcm44aWmjcS/2L6M6CQEwehclRRgre25m4uyOvHnp9dBuMWD0vIlsGom7q3vZB7Gj05kJ7Mn/E3KCwa/ottEbm5/hkd630LJh0dwQcelxdJ7aDV/djBxSeGPgSzy3+HlyC3MAWBoPbi4uvDvw3XLbcyzlJA/80wtc4KfH/g+A4cMBrrdLG4WoKnmmUMsUFED/t58EoI3pBlxdSvntXlx2E1g6kQddVvHbQ1OJmTiD23qVfXVx5ZUwbFiZm6su03i7tn9/I7O8eM1omHIAgHvm3kOjtxoxY24cH35ow3NWwt+HNnDRlK7gUghLJwEQnnE3LlMPoQp8ueuXB5i75R9+3fkHl438kbAPw8g0neF0YQwZhYmM/nO0NSFwuhsAUzZO4cstX1vPobVmXcwRrv/oaUYuGMnohaNpPdXoVjqkyWguiQxwaJuFqAyltXZ2DNUWHR2tN2/e7OwwHCYhK4Fm7zUzVo5exe93/8b1VweUu8/338Pdd8Nff5V8j8FRRjwSy+zm4Xx25QJG9jPG5Hn3XXg2pSF4ZRiV1j0Df7/EyGdOM/6ZYFo0rv4MMOnp0KBB+Vc3G4/sp9d3xjAdbsevZmzzpbw+9RhHtrYiNVVx6eOfYxr0yPk7rh7HXdE38v3eL8D/JJ3i32XMY4H0vjiYjldug1HdAch9MZftp7dzy/RHiDftPO8wLjnNKHjzlLxsJpxGKbVFa11qrw9JCrXI5L+/5amV9wPwVY+dPDCka4X7aA0xMcatIWcoKIB164wrkHNmz4YRqztC0IHzdzg0kEV3L6JnnzNMXfcVY/o+jre7Nxn5GUzdOJWIJhHceNGNpZ7r45/38MS30xjfbzyvPNu01Dqv/vE5L2+2/MI/diVL7v2T/n29MZvBzXLLbNYsuP+Vv8n9zyBwt4zddOpido/eSufORpKNiCj5sl56OnR8fCwJ7d/CXftQYDaByQM8s2DleNx8szAteh0aH2LbsnZEdfGu4ndSCNuRpFAH5OaCz5M9jeGxv16NzgxxdkjVtmEDXDqjMwTvxeVsA8weGSUrvHuSS/47mY0eb9Ew52Kydw3A1Osd6+YtI7fQPaQ78/fPx6zNDOs4jHnrdzP8j0vAPQ+PbU+SP/dDwBjo73j6cVo3as2yLbEM+D0cAJ+zrch47QiuZfy5rjWs2XuIvr+0B6DgpQLcXMp/BPfDrALu3GfMQseOu+iZ9g6P3NmMm26CgAAjOUZEGMtCOJMkhVouMz+Ti98ZxuHCFbDkLbIXP4dP2ePR1XhJSRD8VnPwP8VNYY/y6/FPKtxHpYWjT0Xh0nIjTZt4kLP7GtLbfgHA4xGv8sWSFRQEbcMzryV5HnHMvvQEfS/zIerlBzgd8g1XBo7g78P/gG8iN5vm8cZTnejQrGUFZ4XFy/Po0EERHupZYV2TCd7+6hDr9sQyY8LVNG5c8fdCCGeQpFDLPfzd63x55CXIDKH/vt0s+732/7bxeNWbAp3HgScOMn//PCZ8sYms8FmQ0s6YKAbo0fRS3rnyM5YeXMVrQx5n3TpFn2emwuDRxkHMrpARCgHGgH/juk6jfaPOPLDqytJPmtyB+4O+4OuX+zqiiULUWJIUarGMDGg8pjeFzTbw48Wp3DTEH3fn9OC0qY82fMR///ovppdMuLq4kp+vueHewyxZ5ApDH8Yv7DALH5pBn1Z9Suz3zKuxvG9qx8AmDzHnwWm0uyyGU92eoltYG7a99glag8//Lia/UbFR/jaPBL8Ejn84k5ZNnfuynBA1gSSFWqrQXMh/v/qeaSfv4+6W45nxwCvODsnutK74vYiUnBQaeTfCRblw8iSkpZV8kJ6RWcjBAy7c/uoc7rvDn2vbXYOHB3St+Lm8EPVCeUlBXl6rwX5cu5ZpJ+8D4L/X3OTcYBykMi/KBfoEWpebNzc+xTXwd6VHDzgw7z82jk6Iuk96StdgX/5sTGvZ6cz/iG5Z/pvLQghhC/UyKWgNT/70AU/Mfs3ZoZQr/sxpAP6ZNN7JkQgh6ot6mRTe+uIQU/b/j49jxnMy45S1/ONFi7jz04mk5KSWs3fZkpLglhGF7DyYUq39tdZknc1i4IxrURNcONxuDAANPGUsfSGEY9TLpBDafZd1udt7A3hr6Wfs3V/AE6v+ww8JL9LmjcvYGr+71H2XbDnIkr0lJ6PPyM9gV8IufvzrCL9EuNHthyAW7F/AiYwTlY5pzMxv8HylARe915MlRxeDMjoAhBX2r9TIm0IIYQv1svfR3qS9fLf5NyaND4QeX0DINnwS+5ITvIr2GQ9z0GcGuBQy76YlDI3sZ90vO6cQv9cCwSudmMf2s/1EDP+b+yqn1NYyz3WR+wB+uv9DuoV0Pm9bam4qWms8CgNp8PTl0PIf67arYvbw4NCLuO5aFwIDz9tVCCGqTbqklmH1ajidaOLWRZeDbwKRzSJZ9eT3/LIgk4e2dgGvdPo1/Q8zbv+QYN9gRn3+Nd8mPXb+gY72I/TMXcRnHKdBYD6tG7XEzR3iUpNIbPUReGXQ1q0vw7sM4s1hT+Hm4kZ+PgQ814O8xlsZ7vMuc3LGQEJX/JL78fnYa7i9+w0X8J0RQoiySVKowFdfQbt2JQdt+/THWJ76+z7yQ1YCoMzuaJcCvM/0IOLMc2xp8iTB6UOYfMOb3DokEBeX82/xaA1f/pDE6HnjyG+xBAKOA+CdH45rXjBZDUvehnq665u8dcPYOvFymhCi5pKkUE1Hj2raPPEEXDINEiPwyunIrHs/YnDvFuzbB507V65fvdkMmzZpnvpsLvtz1nKmMB7C1oB3CtGZr3FVeH98/TT/92BUxfMjCCHEBZKkcAEmf5HMTwe+4tdxo2jeuKFNjpmVBTH7zCRlpTK4X5BNjimEEJVVq5KCUuo6YDLgCnyptZ5UVt26PsyFEELYQ3lJoUZ1SVVKuQIfA4OACOB2pZSTpocRQoj6p0YlBeAS4JDW+ojW+izwE2DLGYOFEEKUo6YlhRZAXLH1eEuZlVJqpFJqs1Jqc1JSkkODE0KIuq6mJYUKaa0/11pHa62jmzRp4uxwhBCiTqlpSeEEUHyOxFBLmRBCCAeoaUlhE9BeKdVaKeUB3AbMd3JMQghRb9SoSXa01ial1BPAIowuqV9rrfc4OSwhhKg3alRSANBaLwQWOjsOIYSoj2rcy2tVoZRKAmKdHce/BAHJzg7Cwepjm6F+tlvaXDe00lqX2lOnVieFmkgptbmsNwXrqvrYZqif7ZY213017UGzEEIIJ5KkIIQQwkqSgu197uwAnKA+thnqZ7ulzXWcPFMQQghhJVcKQgghrCQpCCGEsJKkUAGlVEul1Aql1F6l1B6l1JOW8sZKqSVKqYOWr40s5UopNUUpdUgptVMp1f1fx2uglIpXSk11Rnsqy5btVkq9bTlGjKVOJSYxdbxqtPkipdQ/Sql8pdSYio5TE9mqzZZtAUqpX5RS+yw/68uc0aaKVKPNd1r+Te9SSq1TSnUrdqzrlFL7Lf/uxzqrTTaltZZPOR8gBOhuWfYHDmBMAPQ2MNZSPhZ4y7I8GPgTUMClwIZ/HW8y8AMw1dltc0S7gcuBtRjDlrgC/wD9nN0+G7U5GOgJvAGMqeg4zm6fPdts2TYdeMiy7AEEOLt9Nmrz5UAjy/KgYv+2XYHDQBtLe3fU1J9zVT5ypVABrfUprfVWy3ImEIMxx8MwjP8EWL7eaFkeBszQhvVAgFIqBEAp1QNoCix2XAuqx4bt1oAXxn8aT8AdSHBUO6qiqm3WWidqrTcBBZU8To1jqzYrpRoCfYGvLPXOaq3THNCEKqtGm9dprc9YytdjjN4MdXRSMEkKVaCUCgcuBjYATbXWpyybTmP8socyJgpSSrkA7wElLrlrgwtpt9b6H2AFcMryWaS1jnFE3Beikm2u6nFqtAtsc2sgCfhGKbVNKfWlUsrXbsHaSDXa/CDGFTFUYlKw2kiSQiUppfyAX4GntNYZxbdp41qyor69jwELtdbxdgrRLi603UqpdkAnjL+uWgD9lVJ97BSuTdjgZ13hcWoaG7TZDegOfKK1vhjIxrgFU2NVtc1KqaswksLzDgvSCSQpVIJSyh3jH89MrfVvluKEYreFQoBES3lZEwVdBjyhlDoGvAvco5Sa5IDwq81G7R4OrNdaZ2mtszD+yqqRDyChym2u6nFqJBu1OR6I11qfuyL6BSNJ1EhVbbNSKhL4EhimtU6xFNfJScEkKVTA0lPmKyBGa/1+sU3zgXsty/cC84qV32PpjXMpkG65h3mn1jpMax2OcQtphta6xv4lZat2A8eBK5VSbpb/iFdi3MOtcarR5qoep8axVZu11qeBOKVUR0vR1cBeG4drE1Vts1IqDPgNuFtrfaBY/bo5KZizn3TX9A/QG+Myciew3fIZDAQCy4CDwFKgsaW+Aj7G6JWwC4gu5Zj3UfN7H9mk3Rg9ND7DSAR7gfed3TYbtrkZxl/IGUCaZblBWcdxdvvs2WbLtihgs+VYc7H02Klpn2q0+UvgTLG6m4sdazBG76XDwIvObpstPjLMhRBCCCu5fSSEEMJKkoIQQggrSQpCCCGsJCkIIYSwkqQghBDCys3ZAQhRWyilCjG627oDJmAG8IHW2uzUwISwIUkKQlRertY6CkApFYwx2m0D4GVnBiWELcntIyGqQWudCIzEGLpEKaXClVKrlVJbLZ/LAZRSM5RSN57bTyk1Uyk1TCnVWSm1USm13TJWf3snNUWIEuTlNSEqSSmVpbX2+1dZGtARyATMWus8yy/4H7XW0UqpK4GntdY3WoaX3g60Bz7AGBNqpmWIBFetda4j2yNEaeT2kRC24Q5MVUpFAYVABwCt9d9KqWlKqSbATcCvWmuTUuof4EWlVCjwm9b6oLMCF6I4uX0kRDUppdpgJIBE4GmMyYO6AdEYkwqdMwO4C7gf+BpAa/0DMBTIBRYqpfo7LnIhyiZXCkJUg+Uv/08xBjbUlltD8Vprs1LqXoyBAM/5FtgInNZa77Xs3wY4orWeYhmFMxJY7tBGCFEKSQpCVJ63Umo7RV1SvwPODb08DfhVKXUP8BfGJDMAaK0TlFIxGCOHnnMrcLdSqgBjlq+Jdo9eiEqQB81C2JlSygfj/YbuWut0Z8cjRHnkmYIQdqSUGoAxl8RHkhBEbSBXCkIIIazkSkEIIYSVJAUhhBBWkhSEEEJYSVIQQghhJUlBCCGE1f8DCiQQy430g6AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot true/pred prices graph\n",
        "plot_graph(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0342d0",
      "metadata": {
        "id": "df0342d0",
        "outputId": "23d7d81f-22df-4baa-c96b-09fe5eb11bca"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>adjclose</th>\n",
              "      <th>volume</th>\n",
              "      <th>ticker</th>\n",
              "      <th>adjclose_3</th>\n",
              "      <th>true_adjclose_3</th>\n",
              "      <th>buy_profit</th>\n",
              "      <th>sell_profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2002-07-26</th>\n",
              "      <td>10.15</td>\n",
              "      <td>10.15</td>\n",
              "      <td>10.15</td>\n",
              "      <td>10.15</td>\n",
              "      <td>8.563499</td>\n",
              "      <td>0</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>8.670517</td>\n",
              "      <td>10.124336</td>\n",
              "      <td>1.560837</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-07-30</th>\n",
              "      <td>11.25</td>\n",
              "      <td>11.85</td>\n",
              "      <td>11.25</td>\n",
              "      <td>11.50</td>\n",
              "      <td>9.702489</td>\n",
              "      <td>8900</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>9.466700</td>\n",
              "      <td>9.955597</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.253108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-08-01</th>\n",
              "      <td>11.80</td>\n",
              "      <td>11.90</td>\n",
              "      <td>11.80</td>\n",
              "      <td>11.80</td>\n",
              "      <td>9.955597</td>\n",
              "      <td>4300</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>10.456723</td>\n",
              "      <td>10.124336</td>\n",
              "      <td>0.168739</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-08-02</th>\n",
              "      <td>11.80</td>\n",
              "      <td>11.80</td>\n",
              "      <td>11.80</td>\n",
              "      <td>11.80</td>\n",
              "      <td>9.955597</td>\n",
              "      <td>0</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>10.669705</td>\n",
              "      <td>10.166521</td>\n",
              "      <td>0.210924</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-08-13</th>\n",
              "      <td>12.20</td>\n",
              "      <td>12.20</td>\n",
              "      <td>12.10</td>\n",
              "      <td>12.20</td>\n",
              "      <td>10.293074</td>\n",
              "      <td>3900</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>10.778397</td>\n",
              "      <td>10.250890</td>\n",
              "      <td>-0.042184</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-08-14</th>\n",
              "      <td>12.15</td>\n",
              "      <td>12.20</td>\n",
              "      <td>12.15</td>\n",
              "      <td>12.20</td>\n",
              "      <td>10.293074</td>\n",
              "      <td>3700</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>10.779782</td>\n",
              "      <td>9.829041</td>\n",
              "      <td>-0.464032</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-08-30</th>\n",
              "      <td>10.30</td>\n",
              "      <td>10.30</td>\n",
              "      <td>10.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>8.436941</td>\n",
              "      <td>4200</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>8.905578</td>\n",
              "      <td>7.846358</td>\n",
              "      <td>-0.590583</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-09-09</th>\n",
              "      <td>9.75</td>\n",
              "      <td>9.90</td>\n",
              "      <td>9.70</td>\n",
              "      <td>9.80</td>\n",
              "      <td>8.268207</td>\n",
              "      <td>26000</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>8.406970</td>\n",
              "      <td>8.352575</td>\n",
              "      <td>0.084369</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-09-10</th>\n",
              "      <td>9.90</td>\n",
              "      <td>9.95</td>\n",
              "      <td>9.82</td>\n",
              "      <td>9.90</td>\n",
              "      <td>8.352575</td>\n",
              "      <td>36200</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>8.597321</td>\n",
              "      <td>8.386324</td>\n",
              "      <td>0.033749</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-09-12</th>\n",
              "      <td>10.10</td>\n",
              "      <td>10.10</td>\n",
              "      <td>9.90</td>\n",
              "      <td>9.90</td>\n",
              "      <td>8.352575</td>\n",
              "      <td>4000</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>8.937477</td>\n",
              "      <td>8.352575</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-09-17</th>\n",
              "      <td>9.75</td>\n",
              "      <td>9.90</td>\n",
              "      <td>9.75</td>\n",
              "      <td>9.90</td>\n",
              "      <td>8.352575</td>\n",
              "      <td>4800</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>8.965299</td>\n",
              "      <td>7.930729</td>\n",
              "      <td>-0.421846</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-09-26</th>\n",
              "      <td>9.70</td>\n",
              "      <td>9.70</td>\n",
              "      <td>9.70</td>\n",
              "      <td>9.70</td>\n",
              "      <td>8.183837</td>\n",
              "      <td>1000</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>8.563069</td>\n",
              "      <td>8.015098</td>\n",
              "      <td>-0.168739</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-10-14</th>\n",
              "      <td>8.95</td>\n",
              "      <td>9.00</td>\n",
              "      <td>8.90</td>\n",
              "      <td>8.99</td>\n",
              "      <td>7.584813</td>\n",
              "      <td>13500</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>8.071711</td>\n",
              "      <td>6.960480</td>\n",
              "      <td>-0.624333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-10-28</th>\n",
              "      <td>7.50</td>\n",
              "      <td>7.50</td>\n",
              "      <td>7.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>5.905862</td>\n",
              "      <td>3200</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>6.540550</td>\n",
              "      <td>6.327710</td>\n",
              "      <td>0.421848</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-11-13</th>\n",
              "      <td>7.90</td>\n",
              "      <td>8.38</td>\n",
              "      <td>7.90</td>\n",
              "      <td>8.38</td>\n",
              "      <td>7.070161</td>\n",
              "      <td>38300</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>7.315137</td>\n",
              "      <td>7.171404</td>\n",
              "      <td>0.101243</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-11-15</th>\n",
              "      <td>8.50</td>\n",
              "      <td>8.50</td>\n",
              "      <td>8.40</td>\n",
              "      <td>8.40</td>\n",
              "      <td>7.087034</td>\n",
              "      <td>3400</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>7.710267</td>\n",
              "      <td>7.087034</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-12-04</th>\n",
              "      <td>10.05</td>\n",
              "      <td>10.25</td>\n",
              "      <td>10.05</td>\n",
              "      <td>10.05</td>\n",
              "      <td>8.479132</td>\n",
              "      <td>2400</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>9.407278</td>\n",
              "      <td>8.816609</td>\n",
              "      <td>0.337478</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-12-05</th>\n",
              "      <td>10.05</td>\n",
              "      <td>10.50</td>\n",
              "      <td>10.05</td>\n",
              "      <td>10.50</td>\n",
              "      <td>8.858794</td>\n",
              "      <td>3500</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>9.425570</td>\n",
              "      <td>8.943163</td>\n",
              "      <td>0.084369</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-12-11</th>\n",
              "      <td>10.51</td>\n",
              "      <td>10.65</td>\n",
              "      <td>10.51</td>\n",
              "      <td>10.60</td>\n",
              "      <td>8.943163</td>\n",
              "      <td>4100</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>9.556677</td>\n",
              "      <td>9.322825</td>\n",
              "      <td>0.379663</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002-12-13</th>\n",
              "      <td>10.95</td>\n",
              "      <td>11.05</td>\n",
              "      <td>10.80</td>\n",
              "      <td>11.00</td>\n",
              "      <td>9.280639</td>\n",
              "      <td>5200</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>9.728291</td>\n",
              "      <td>9.322825</td>\n",
              "      <td>0.042187</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             open   high    low  close   adjclose  volume ticker  adjclose_3  \\\n",
              "2002-07-26  10.15  10.15  10.15  10.15   8.563499       0   NDAQ    8.670517   \n",
              "2002-07-30  11.25  11.85  11.25  11.50   9.702489    8900   NDAQ    9.466700   \n",
              "2002-08-01  11.80  11.90  11.80  11.80   9.955597    4300   NDAQ   10.456723   \n",
              "2002-08-02  11.80  11.80  11.80  11.80   9.955597       0   NDAQ   10.669705   \n",
              "2002-08-13  12.20  12.20  12.10  12.20  10.293074    3900   NDAQ   10.778397   \n",
              "2002-08-14  12.15  12.20  12.15  12.20  10.293074    3700   NDAQ   10.779782   \n",
              "2002-08-30  10.30  10.30  10.00  10.00   8.436941    4200   NDAQ    8.905578   \n",
              "2002-09-09   9.75   9.90   9.70   9.80   8.268207   26000   NDAQ    8.406970   \n",
              "2002-09-10   9.90   9.95   9.82   9.90   8.352575   36200   NDAQ    8.597321   \n",
              "2002-09-12  10.10  10.10   9.90   9.90   8.352575    4000   NDAQ    8.937477   \n",
              "2002-09-17   9.75   9.90   9.75   9.90   8.352575    4800   NDAQ    8.965299   \n",
              "2002-09-26   9.70   9.70   9.70   9.70   8.183837    1000   NDAQ    8.563069   \n",
              "2002-10-14   8.95   9.00   8.90   8.99   7.584813   13500   NDAQ    8.071711   \n",
              "2002-10-28   7.50   7.50   7.00   7.00   5.905862    3200   NDAQ    6.540550   \n",
              "2002-11-13   7.90   8.38   7.90   8.38   7.070161   38300   NDAQ    7.315137   \n",
              "2002-11-15   8.50   8.50   8.40   8.40   7.087034    3400   NDAQ    7.710267   \n",
              "2002-12-04  10.05  10.25  10.05  10.05   8.479132    2400   NDAQ    9.407278   \n",
              "2002-12-05  10.05  10.50  10.05  10.50   8.858794    3500   NDAQ    9.425570   \n",
              "2002-12-11  10.51  10.65  10.51  10.60   8.943163    4100   NDAQ    9.556677   \n",
              "2002-12-13  10.95  11.05  10.80  11.00   9.280639    5200   NDAQ    9.728291   \n",
              "\n",
              "            true_adjclose_3  buy_profit  sell_profit  \n",
              "2002-07-26        10.124336    1.560837     0.000000  \n",
              "2002-07-30         9.955597    0.000000    -0.253108  \n",
              "2002-08-01        10.124336    0.168739     0.000000  \n",
              "2002-08-02        10.166521    0.210924     0.000000  \n",
              "2002-08-13        10.250890   -0.042184     0.000000  \n",
              "2002-08-14         9.829041   -0.464032     0.000000  \n",
              "2002-08-30         7.846358   -0.590583     0.000000  \n",
              "2002-09-09         8.352575    0.084369     0.000000  \n",
              "2002-09-10         8.386324    0.033749     0.000000  \n",
              "2002-09-12         8.352575    0.000000     0.000000  \n",
              "2002-09-17         7.930729   -0.421846     0.000000  \n",
              "2002-09-26         8.015098   -0.168739     0.000000  \n",
              "2002-10-14         6.960480   -0.624333     0.000000  \n",
              "2002-10-28         6.327710    0.421848     0.000000  \n",
              "2002-11-13         7.171404    0.101243     0.000000  \n",
              "2002-11-15         7.087034    0.000000     0.000000  \n",
              "2002-12-04         8.816609    0.337478     0.000000  \n",
              "2002-12-05         8.943163    0.084369     0.000000  \n",
              "2002-12-11         9.322825    0.379663     0.000000  \n",
              "2002-12-13         9.322825    0.042187     0.000000  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668f3cd2",
      "metadata": {
        "id": "668f3cd2",
        "outputId": "865755fc-8e3c-4c0f-dba2-0dcaacf4269a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>adjclose</th>\n",
              "      <th>volume</th>\n",
              "      <th>ticker</th>\n",
              "      <th>adjclose_3</th>\n",
              "      <th>true_adjclose_3</th>\n",
              "      <th>buy_profit</th>\n",
              "      <th>sell_profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-12-02</th>\n",
              "      <td>200.020004</td>\n",
              "      <td>203.080002</td>\n",
              "      <td>198.100006</td>\n",
              "      <td>201.229996</td>\n",
              "      <td>199.803787</td>\n",
              "      <td>940700</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>199.734787</td>\n",
              "      <td>204.093170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.289383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-12-03</th>\n",
              "      <td>203.080002</td>\n",
              "      <td>203.910004</td>\n",
              "      <td>196.149994</td>\n",
              "      <td>200.119995</td>\n",
              "      <td>198.701645</td>\n",
              "      <td>882800</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>198.671997</td>\n",
              "      <td>205.920135</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.218491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-01-10</th>\n",
              "      <td>191.839996</td>\n",
              "      <td>192.949997</td>\n",
              "      <td>185.860001</td>\n",
              "      <td>189.119995</td>\n",
              "      <td>187.779617</td>\n",
              "      <td>1776800</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>191.508972</td>\n",
              "      <td>185.893082</td>\n",
              "      <td>-1.886536</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-08</th>\n",
              "      <td>177.770004</td>\n",
              "      <td>178.520004</td>\n",
              "      <td>174.679993</td>\n",
              "      <td>177.860001</td>\n",
              "      <td>176.599442</td>\n",
              "      <td>727300</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>176.419876</td>\n",
              "      <td>171.148331</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.451111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-11</th>\n",
              "      <td>177.190002</td>\n",
              "      <td>177.589996</td>\n",
              "      <td>171.880005</td>\n",
              "      <td>172.369995</td>\n",
              "      <td>171.148331</td>\n",
              "      <td>1101600</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>175.145462</td>\n",
              "      <td>173.422104</td>\n",
              "      <td>2.273773</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-24</th>\n",
              "      <td>161.309998</td>\n",
              "      <td>170.960007</td>\n",
              "      <td>161.000000</td>\n",
              "      <td>170.190002</td>\n",
              "      <td>168.983795</td>\n",
              "      <td>1201800</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>167.412094</td>\n",
              "      <td>168.785202</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.198593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-28</th>\n",
              "      <td>170.619995</td>\n",
              "      <td>172.759995</td>\n",
              "      <td>169.589996</td>\n",
              "      <td>171.149994</td>\n",
              "      <td>169.936981</td>\n",
              "      <td>1089000</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>168.015778</td>\n",
              "      <td>171.257553</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.320572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-03-03</th>\n",
              "      <td>173.289993</td>\n",
              "      <td>173.710007</td>\n",
              "      <td>171.089996</td>\n",
              "      <td>172.479996</td>\n",
              "      <td>171.257553</td>\n",
              "      <td>577600</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>169.329498</td>\n",
              "      <td>163.830566</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.426987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-03-10</th>\n",
              "      <td>166.360001</td>\n",
              "      <td>167.300003</td>\n",
              "      <td>162.580002</td>\n",
              "      <td>166.479996</td>\n",
              "      <td>165.830994</td>\n",
              "      <td>879200</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>165.848831</td>\n",
              "      <td>168.918915</td>\n",
              "      <td>3.087921</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-03-16</th>\n",
              "      <td>170.779999</td>\n",
              "      <td>174.100006</td>\n",
              "      <td>169.419998</td>\n",
              "      <td>173.320007</td>\n",
              "      <td>172.644333</td>\n",
              "      <td>585800</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>166.715332</td>\n",
              "      <td>176.947495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.303162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-03-17</th>\n",
              "      <td>173.300003</td>\n",
              "      <td>176.149994</td>\n",
              "      <td>172.899994</td>\n",
              "      <td>176.100006</td>\n",
              "      <td>175.413498</td>\n",
              "      <td>512900</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>168.711823</td>\n",
              "      <td>176.817993</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.404495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-03-18</th>\n",
              "      <td>176.639999</td>\n",
              "      <td>179.919998</td>\n",
              "      <td>176.190002</td>\n",
              "      <td>179.229996</td>\n",
              "      <td>178.531296</td>\n",
              "      <td>949600</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>171.130890</td>\n",
              "      <td>173.461136</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.070160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-04-04</th>\n",
              "      <td>182.229996</td>\n",
              "      <td>183.350006</td>\n",
              "      <td>181.000000</td>\n",
              "      <td>182.360001</td>\n",
              "      <td>181.649094</td>\n",
              "      <td>874100</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>178.543091</td>\n",
              "      <td>182.286591</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.637497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-04-07</th>\n",
              "      <td>182.850006</td>\n",
              "      <td>183.860001</td>\n",
              "      <td>181.630005</td>\n",
              "      <td>183.000000</td>\n",
              "      <td>182.286591</td>\n",
              "      <td>659200</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>180.008850</td>\n",
              "      <td>179.358047</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.928543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-05-04</th>\n",
              "      <td>157.800003</td>\n",
              "      <td>160.399994</td>\n",
              "      <td>156.350006</td>\n",
              "      <td>160.100006</td>\n",
              "      <td>159.475876</td>\n",
              "      <td>1454300</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>156.686478</td>\n",
              "      <td>145.610123</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.865753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-05-06</th>\n",
              "      <td>153.919998</td>\n",
              "      <td>154.220001</td>\n",
              "      <td>150.570007</td>\n",
              "      <td>152.289993</td>\n",
              "      <td>151.696304</td>\n",
              "      <td>675000</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>154.667557</td>\n",
              "      <td>142.671631</td>\n",
              "      <td>-9.024673</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-05-09</th>\n",
              "      <td>150.589996</td>\n",
              "      <td>150.649994</td>\n",
              "      <td>145.699997</td>\n",
              "      <td>146.179993</td>\n",
              "      <td>145.610123</td>\n",
              "      <td>736200</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>152.161865</td>\n",
              "      <td>143.767349</td>\n",
              "      <td>-1.842773</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-05-17</th>\n",
              "      <td>148.639999</td>\n",
              "      <td>149.259995</td>\n",
              "      <td>147.330002</td>\n",
              "      <td>148.789993</td>\n",
              "      <td>148.209946</td>\n",
              "      <td>967500</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>144.924866</td>\n",
              "      <td>145.560333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.649612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-05-19</th>\n",
              "      <td>140.899994</td>\n",
              "      <td>143.770004</td>\n",
              "      <td>140.309998</td>\n",
              "      <td>143.080002</td>\n",
              "      <td>142.522217</td>\n",
              "      <td>1105100</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>143.405258</td>\n",
              "      <td>146.058380</td>\n",
              "      <td>3.536163</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-06-03</th>\n",
              "      <td>154.100006</td>\n",
              "      <td>154.479996</td>\n",
              "      <td>152.729996</td>\n",
              "      <td>154.070007</td>\n",
              "      <td>153.469391</td>\n",
              "      <td>582700</td>\n",
              "      <td>NDAQ</td>\n",
              "      <td>152.223694</td>\n",
              "      <td>153.309998</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.159393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  open        high         low       close    adjclose  \\\n",
              "2021-12-02  200.020004  203.080002  198.100006  201.229996  199.803787   \n",
              "2021-12-03  203.080002  203.910004  196.149994  200.119995  198.701645   \n",
              "2022-01-10  191.839996  192.949997  185.860001  189.119995  187.779617   \n",
              "2022-02-08  177.770004  178.520004  174.679993  177.860001  176.599442   \n",
              "2022-02-11  177.190002  177.589996  171.880005  172.369995  171.148331   \n",
              "2022-02-24  161.309998  170.960007  161.000000  170.190002  168.983795   \n",
              "2022-02-28  170.619995  172.759995  169.589996  171.149994  169.936981   \n",
              "2022-03-03  173.289993  173.710007  171.089996  172.479996  171.257553   \n",
              "2022-03-10  166.360001  167.300003  162.580002  166.479996  165.830994   \n",
              "2022-03-16  170.779999  174.100006  169.419998  173.320007  172.644333   \n",
              "2022-03-17  173.300003  176.149994  172.899994  176.100006  175.413498   \n",
              "2022-03-18  176.639999  179.919998  176.190002  179.229996  178.531296   \n",
              "2022-04-04  182.229996  183.350006  181.000000  182.360001  181.649094   \n",
              "2022-04-07  182.850006  183.860001  181.630005  183.000000  182.286591   \n",
              "2022-05-04  157.800003  160.399994  156.350006  160.100006  159.475876   \n",
              "2022-05-06  153.919998  154.220001  150.570007  152.289993  151.696304   \n",
              "2022-05-09  150.589996  150.649994  145.699997  146.179993  145.610123   \n",
              "2022-05-17  148.639999  149.259995  147.330002  148.789993  148.209946   \n",
              "2022-05-19  140.899994  143.770004  140.309998  143.080002  142.522217   \n",
              "2022-06-03  154.100006  154.479996  152.729996  154.070007  153.469391   \n",
              "\n",
              "             volume ticker  adjclose_3  true_adjclose_3  buy_profit  \\\n",
              "2021-12-02   940700   NDAQ  199.734787       204.093170    0.000000   \n",
              "2021-12-03   882800   NDAQ  198.671997       205.920135    0.000000   \n",
              "2022-01-10  1776800   NDAQ  191.508972       185.893082   -1.886536   \n",
              "2022-02-08   727300   NDAQ  176.419876       171.148331    0.000000   \n",
              "2022-02-11  1101600   NDAQ  175.145462       173.422104    2.273773   \n",
              "2022-02-24  1201800   NDAQ  167.412094       168.785202    0.000000   \n",
              "2022-02-28  1089000   NDAQ  168.015778       171.257553    0.000000   \n",
              "2022-03-03   577600   NDAQ  169.329498       163.830566    0.000000   \n",
              "2022-03-10   879200   NDAQ  165.848831       168.918915    3.087921   \n",
              "2022-03-16   585800   NDAQ  166.715332       176.947495    0.000000   \n",
              "2022-03-17   512900   NDAQ  168.711823       176.817993    0.000000   \n",
              "2022-03-18   949600   NDAQ  171.130890       173.461136    0.000000   \n",
              "2022-04-04   874100   NDAQ  178.543091       182.286591    0.000000   \n",
              "2022-04-07   659200   NDAQ  180.008850       179.358047    0.000000   \n",
              "2022-05-04  1454300   NDAQ  156.686478       145.610123    0.000000   \n",
              "2022-05-06   675000   NDAQ  154.667557       142.671631   -9.024673   \n",
              "2022-05-09   736200   NDAQ  152.161865       143.767349   -1.842773   \n",
              "2022-05-17   967500   NDAQ  144.924866       145.560333    0.000000   \n",
              "2022-05-19  1105100   NDAQ  143.405258       146.058380    3.536163   \n",
              "2022-06-03   582700   NDAQ  152.223694       153.309998    0.000000   \n",
              "\n",
              "            sell_profit  \n",
              "2021-12-02    -4.289383  \n",
              "2021-12-03    -7.218491  \n",
              "2022-01-10     0.000000  \n",
              "2022-02-08     5.451111  \n",
              "2022-02-11     0.000000  \n",
              "2022-02-24     0.198593  \n",
              "2022-02-28    -1.320572  \n",
              "2022-03-03     7.426987  \n",
              "2022-03-10     0.000000  \n",
              "2022-03-16    -4.303162  \n",
              "2022-03-17    -1.404495  \n",
              "2022-03-18     5.070160  \n",
              "2022-04-04    -0.637497  \n",
              "2022-04-07     2.928543  \n",
              "2022-05-04    13.865753  \n",
              "2022-05-06     0.000000  \n",
              "2022-05-09     0.000000  \n",
              "2022-05-17     2.649612  \n",
              "2022-05-19     0.000000  \n",
              "2022-06-03     0.159393  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_df.tail(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e714f4",
      "metadata": {
        "id": "03e714f4",
        "outputId": "bfc439d3-73e1-4504-a8fb-a9cf23ab740c"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "cannot assign to operator (<ipython-input-59-87357480c861>, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-59-87357480c861>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tensorboard --logdir=\"logs\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to operator\n"
          ]
        }
      ],
      "source": [
        "tensorboard --logdir=\"logs\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4108833d",
      "metadata": {
        "id": "4108833d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "RNN - Nasdaq.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}